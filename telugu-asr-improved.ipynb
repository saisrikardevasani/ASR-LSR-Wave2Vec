{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":10729776,"datasetId":6652210,"databundleVersionId":11080209}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\n=============================================================================\n  Improved Telugu ASR Fine-Tuning Pipeline\n  - IndicWav2Vec / WavLM / XLSR-1B base models\n  - Frozen feature encoder\n  - Proper CTC training with SpecAugment\n  - KenLM n-gram language model + beam search decoding\n  - Whisper fine-tuning comparison\n  - Proper evaluation with WER and CER\n=============================================================================\n  INSTRUCTIONS:\n  - Run this on Kaggle/Colab with a GPU (T4 minimum, A100 preferred)\n  - Adjust `BASE_PATH` to your dataset location\n  - Set `HF_TOKEN` to your Hugging Face write token\n  - Choose your model in the CONFIG section\n=============================================================================\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:43:27.088179Z","iopub.execute_input":"2026-02-19T00:43:27.088566Z","iopub.status.idle":"2026-02-19T00:43:27.103349Z","shell.execute_reply.started":"2026-02-19T00:43:27.088524Z","shell.execute_reply":"2026-02-19T00:43:27.102403Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'\\n=============================================================================\\n  Improved Telugu ASR Fine-Tuning Pipeline\\n  - IndicWav2Vec / WavLM / XLSR-1B base models\\n  - Frozen feature encoder\\n  - Proper CTC training with SpecAugment\\n  - KenLM n-gram language model + beam search decoding\\n  - Whisper fine-tuning comparison\\n  - Proper evaluation with WER and CER\\n=============================================================================\\n  INSTRUCTIONS:\\n  - Run this on Kaggle/Colab with a GPU (T4 minimum, A100 preferred)\\n  - Adjust `BASE_PATH` to your dataset location\\n  - Set `HF_TOKEN` to your Hugging Face write token\\n  - Choose your model in the CONFIG section\\n=============================================================================\\n'"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"# üîß Section 1: Setup & Installation\n","metadata":{}},{"cell_type":"code","source":"# !pip install -q transformers datasets accelerate evaluate jiwer\n# !pip install -q pyctcdecode kenlm\n# !pip install -q librosa soundfile torchaudio\n# !pip install -q sentencepiece\n# !pip install -q huggingface_hub\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:43:27.104647Z","iopub.execute_input":"2026-02-19T00:43:27.105024Z","iopub.status.idle":"2026-02-19T00:43:27.118880Z","shell.execute_reply.started":"2026-02-19T00:43:27.104984Z","shell.execute_reply":"2026-02-19T00:43:27.117933Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# ‚öôÔ∏è Section 2: Configuration\n","metadata":{}},{"cell_type":"code","source":"import os\n\n# ===================== CONFIGURATION =====================\n# Choose one of the base models:\n#   \"ai4bharat/indicwav2vec_v1_telugu\"        - Best for Telugu (Indian language pre-trained)\n#   \"facebook/wav2vec2-xls-r-1b\"              - 1B param XLSR (strong multilingual)\n#   \"facebook/wav2vec2-xls-r-300m\"            - 300M param XLSR (your current)\n#   \"microsoft/wavlm-large\"                   - WavLM (often better than wav2vec2)\n#   \"facebook/wav2vec2-large-xlsr-53\"          - XLSR-53 (your current)\nBASE_MODEL = \"facebook/wav2vec2-xls-r-300m\"\n\n# Dataset path - CHANGE THIS to match your environment\nBASE_PATH = \"/kaggle/input/datasets/rishiakkiraju/telugu-microsoft-corpus-major-project/telugu_microsoft_corpus/microsoftspeechcorpusindianlanguages\"\n\n# Hugging Face token (for pushing to hub - optional)\nHF_TOKEN = \"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\"  # Set your token here or use huggingface-cli login\n\n# Training config\nNUM_EPOCHS = 30\nLEARNING_RATE = 1e-4\nBATCH_SIZE = 8\nGRADIENT_ACCUMULATION = 4  # effective batch = BATCH_SIZE * GRADIENT_ACCUMULATION = 32\nWARMUP_RATIO = 0.1\nFREEZE_FEATURE_ENCODER = True\nSAMPLING_RATE = 16000\nMAX_AUDIO_LENGTH_SECONDS = 20.0\n\n# Output directories\nOUTPUT_DIR = \"./results_improved\"\nLOGGING_DIR = \"./logs_improved\"\nLM_DIR = \"./language_model\"\n# =========================================================\n\nprint(f\"Base model: {BASE_MODEL}\")\nprint(f\"Dataset path: {BASE_PATH}\")\nprint(f\"Epochs: {NUM_EPOCHS}, LR: {LEARNING_RATE}, Effective batch: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:43:27.120234Z","iopub.execute_input":"2026-02-19T00:43:27.120702Z","iopub.status.idle":"2026-02-19T00:43:27.145340Z","shell.execute_reply.started":"2026-02-19T00:43:27.120656Z","shell.execute_reply":"2026-02-19T00:43:27.144423Z"}},"outputs":[{"name":"stdout","text":"Base model: facebook/wav2vec2-xls-r-300m\nDataset path: /kaggle/input/datasets/rishiakkiraju/telugu-microsoft-corpus-major-project/telugu_microsoft_corpus/microsoftspeechcorpusindianlanguages\nEpochs: 30, LR: 0.0001, Effective batch: 32\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# üìÇ Section 3: Dataset Loading & Exploration\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Define dataset paths\ntrain_path = os.path.join(BASE_PATH, \"te-in-Train\")\ntest_path = os.path.join(BASE_PATH, \"te-in-Test\")\nvalidation_path = os.path.join(BASE_PATH, \"te-in-Measurement\")\n\ndef load_data(data_path, transcription_file, audio_subfolder=\"Audios\"):\n    \"\"\"Load dataset with proper audio path construction.\"\"\"\n    df = pd.read_csv(\n        os.path.join(data_path, transcription_file),\n        delimiter=\",\",\n        encoding=\"utf-8-sig\",  # handles BOM character\n    )\n    df[\"audio_path\"] = df[\"audio_path_file\"].apply(\n        lambda x: os.path.join(data_path, audio_subfolder, x)\n    )\n    return df\n\n# Load all splits\ntrain_df = load_data(train_path, \"train_transcriptions.csv\", audio_subfolder=\"Audios\")\ntest_df = load_data(test_path, \"test_transcriptions.csv\", audio_subfolder=\"Audios\")\nvalidation_df = load_data(\n    validation_path, \"validation_transcriptions.csv\",\n    audio_subfolder=\"Audios_with_Transcriptions\"\n)\n\n# Remove validation files from test set (avoid leakage)\nvalidation_files = set(validation_df[\"audio_path_file\"].tolist())\ntest_df = test_df[~test_df[\"audio_path_file\"].isin(validation_files)].reset_index(drop=True)\n\nprint(f\"Train samples:      {len(train_df):,}\")\nprint(f\"Test samples:       {len(test_df):,}\")\nprint(f\"Validation samples: {len(validation_df):,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:43:27.146561Z","iopub.execute_input":"2026-02-19T00:43:27.146849Z","iopub.status.idle":"2026-02-19T00:43:27.765393Z","shell.execute_reply.started":"2026-02-19T00:43:27.146823Z","shell.execute_reply":"2026-02-19T00:43:27.764127Z"}},"outputs":[{"name":"stdout","text":"Train samples:      44,882\nTest samples:       2,640\nValidation samples: 400\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Quick data exploration\nprint(\"\\n--- Sample transcriptions ---\")\nfor i in range(3):\n    print(f\"  [{i}] {train_df.iloc[i]['transcription']}\")\n\nprint(f\"\\n--- Transcription length stats (characters) ---\")\nlengths = train_df[\"transcription\"].str.len()\nprint(f\"  Mean: {lengths.mean():.0f}, Median: {lengths.median():.0f}, \"\n      f\"Min: {lengths.min()}, Max: {lengths.max()}\")\n\n# Check for any NaN or empty transcriptions\nnan_count = train_df[\"transcription\"].isna().sum()\nempty_count = (train_df[\"transcription\"].str.strip() == \"\").sum()\nprint(f\"\\n--- Data quality ---\")\nprint(f\"  NaN transcriptions: {nan_count}\")\nprint(f\"  Empty transcriptions: {empty_count}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:43:27.768804Z","iopub.execute_input":"2026-02-19T00:43:27.769107Z","iopub.status.idle":"2026-02-19T00:43:27.821623Z","shell.execute_reply.started":"2026-02-19T00:43:27.769079Z","shell.execute_reply":"2026-02-19T00:43:27.820694Z"}},"outputs":[{"name":"stdout","text":"\n--- Sample transcriptions ---\n  [0] ‡∞ï‡∞ö‡±ç‡∞ö‡∞ø‡∞§‡∞Ç‡∞ó‡∞æ ‡∞ö‡±Ç‡∞™‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø ‡∞ï‡∞¶‡∞æ ‡∞Æ‡∞∞‡∞ø\n  [1] ‡∞Ö ‡∞ö‡∞∞‡∞£‡±ç ‡∞ï‡∞¶‡∞æ ‡∞§‡±Ü‡∞≤‡±Å‡∞∏‡±Å\n  [2] ‡∞ö‡∞™‡±ç‡∞™‡∞æ‡∞≤‡∞Ç‡∞ü‡±á ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞â‡∞Ç‡∞ü‡∞æ‡∞Ø‡∞ø ‡∞ó‡∞æ‡∞®‡∞ø\n\n--- Transcription length stats (characters) ---\n  Mean: 41, Median: 30, Min: 1, Max: 260\n\n--- Data quality ---\n  NaN transcriptions: 0\n  Empty transcriptions: 0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# üßπ Section 4: Text Normalization & Vocabulary\n","metadata":{}},{"cell_type":"code","source":"import re\n\ndef normalize_telugu_text(text):\n    \"\"\"\n    Normalize Telugu transcriptions:\n    - Keep Telugu Unicode characters (U+0C00 - U+0C7F)\n    - Keep spaces (word boundaries)\n    - Remove all other characters (English, punctuation, etc.)\n    - Collapse multiple spaces\n    - Strip leading/trailing whitespace\n    \"\"\"\n    if pd.isna(text) or not isinstance(text, str):\n        return \"\"\n    # Keep only Telugu chars and whitespace\n    text = re.sub(r'[^\\u0C00-\\u0C7F\\s]', '', text)\n    # Collapse multiple spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# Apply normalization ONCE (no double replacement)\ntrain_df[\"transcription\"] = train_df[\"transcription\"].apply(normalize_telugu_text)\ntest_df[\"transcription\"] = test_df[\"transcription\"].apply(normalize_telugu_text)\nvalidation_df[\"transcription\"] = validation_df[\"transcription\"].apply(normalize_telugu_text)\n\n# Remove any rows with empty transcriptions after normalization\ntrain_df = train_df[train_df[\"transcription\"].str.len() > 0].reset_index(drop=True)\ntest_df = test_df[test_df[\"transcription\"].str.len() > 0].reset_index(drop=True)\nvalidation_df = validation_df[validation_df[\"transcription\"].str.len() > 0].reset_index(drop=True)\n\nprint(f\"After cleaning - Train: {len(train_df):,}, Test: {len(test_df):,}, Val: {len(validation_df):,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:43:27.822889Z","iopub.execute_input":"2026-02-19T00:43:27.823267Z","iopub.status.idle":"2026-02-19T00:43:28.193484Z","shell.execute_reply.started":"2026-02-19T00:43:27.823230Z","shell.execute_reply":"2026-02-19T00:43:28.192369Z"}},"outputs":[{"name":"stdout","text":"After cleaning - Train: 44,882, Test: 2,640, Val: 400\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Build vocabulary from all splits\nall_text = \" \".join(\n    train_df[\"transcription\"].tolist() +\n    test_df[\"transcription\"].tolist() +\n    validation_df[\"transcription\"].tolist()\n)\nvocab_chars = sorted(set(all_text))\nprint(f\"Unique characters in dataset: {len(vocab_chars)}\")\nprint(f\"Characters: {vocab_chars}\")\n\n# Build vocab dict with special tokens\nvocab_dict = {char: idx for idx, char in enumerate(vocab_chars)}\n# Replace space with word delimiter token |\nvocab_dict[\"|\"] = vocab_dict.pop(\" \")\n# Add special tokens\nvocab_dict[\"[UNK]\"] = len(vocab_dict)\nvocab_dict[\"[PAD]\"] = len(vocab_dict)\n\nprint(f\"\\nVocab size (with special tokens): {len(vocab_dict)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:43:28.194618Z","iopub.execute_input":"2026-02-19T00:43:28.194918Z","iopub.status.idle":"2026-02-19T00:43:28.334156Z","shell.execute_reply.started":"2026-02-19T00:43:28.194881Z","shell.execute_reply":"2026-02-19T00:43:28.333177Z"}},"outputs":[{"name":"stdout","text":"Unique characters in dataset: 65\nCharacters: [' ', '‡∞Ç', '‡∞É', '‡∞Ö', '‡∞Ü', '‡∞á', '‡∞à', '‡∞â', '‡∞ä', '‡∞ã', '‡∞é', '‡∞è', '‡∞ê', '‡∞í', '‡∞ì', '‡∞î', '‡∞ï', '‡∞ñ', '‡∞ó', '‡∞ò', '‡∞ô', '‡∞ö', '‡∞õ', '‡∞ú', '‡∞ù', '‡∞û', '‡∞ü', '‡∞†', '‡∞°', '‡∞¢', '‡∞£', '‡∞§', '‡∞•', '‡∞¶', '‡∞ß', '‡∞®', '‡∞™', '‡∞´', '‡∞¨', '‡∞≠', '‡∞Æ', '‡∞Ø', '‡∞∞', '‡∞≤', '‡∞≥', '‡∞µ', '‡∞∂', '‡∞∑', '‡∞∏', '‡∞π', '‡∞æ', '‡∞ø', '‡±Ä', '‡±Å', '‡±Ç', '‡±É', '‡±Ü', '‡±á', '‡±à', '‡±ä', '‡±ã', '‡±å', '‡±ç', '‡±ñ', '‡±¶']\n\nVocab size (with special tokens): 67\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import json\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nvocab_path = os.path.join(OUTPUT_DIR, \"vocab.json\")\nwith open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(vocab_dict, f, ensure_ascii=False)\n\nprint(f\"Vocabulary saved to {vocab_path}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:43:28.335299Z","iopub.execute_input":"2026-02-19T00:43:28.335860Z","iopub.status.idle":"2026-02-19T00:43:28.351438Z","shell.execute_reply.started":"2026-02-19T00:43:28.335824Z","shell.execute_reply":"2026-02-19T00:43:28.350176Z"}},"outputs":[{"name":"stdout","text":"Vocabulary saved to ./results_improved/vocab.json\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# üî§ Section 5: Tokenizer, Feature Extractor & Processor\n","metadata":{}},{"cell_type":"code","source":"from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n\n# Create tokenizer from our vocabulary\ntokenizer = Wav2Vec2CTCTokenizer(\n    vocab_path,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    word_delimiter_token=\"|\",\n)\n\n# Feature extractor - standard for wav2vec2 / WavLM\nfeature_extractor = Wav2Vec2FeatureExtractor(\n    feature_size=1,\n    sampling_rate=SAMPLING_RATE,\n    padding_value=0.0,\n    do_normalize=True,\n    return_attention_mask=True,\n)\n\n# Combined processor\nprocessor = Wav2Vec2Processor(\n    feature_extractor=feature_extractor,\n    tokenizer=tokenizer,\n)\n\nprint(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:43:28.352548Z","iopub.execute_input":"2026-02-19T00:43:28.352881Z","iopub.status.idle":"2026-02-19T00:43:38.882896Z","shell.execute_reply.started":"2026-02-19T00:43:28.352848Z","shell.execute_reply":"2026-02-19T00:43:38.881924Z"}},"outputs":[{"name":"stderr","text":"2026-02-19 00:43:33.640706: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771461813.667012     151 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771461813.674870     151 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771461813.695669     151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771461813.695705     151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771461813.695708     151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771461813.695711     151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Tokenizer vocab size: 67\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# üìä Section 6: Dataset Preparation (HuggingFace Datasets)\n","metadata":{}},{"cell_type":"code","source":"import librosa\nimport numpy as np\nfrom datasets import Dataset\n\ndef prepare_dataset(batch):\n    \"\"\"\n    Load audio, resample to 16kHz, extract features, and tokenize labels.\n    This runs via Dataset.map() ‚Äî processes one example at a time.\n    \"\"\"\n    try:\n        # Load audio\n        audio, sr = librosa.load(batch[\"audio_path\"], sr=SAMPLING_RATE)\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Error loading {batch['audio_path']}: {e}\")\n        # Return a short silence as fallback\n        audio = np.zeros(SAMPLING_RATE, dtype=np.float32)\n\n    # Truncate to max length\n    max_samples = int(MAX_AUDIO_LENGTH_SECONDS * SAMPLING_RATE)\n    if len(audio) > max_samples:\n        audio = audio[:max_samples]\n\n    # Process audio through feature extractor\n    input_values = processor(\n        audio, sampling_rate=SAMPLING_RATE, return_tensors=\"np\"\n    ).input_values[0]\n\n    batch[\"input_values\"] = input_values\n    batch[\"input_length\"] = len(input_values)\n\n    # Tokenize transcription (updated API ‚Äî no more as_target_processor)\n    batch[\"labels\"] = processor.tokenizer(batch[\"transcription\"]).input_ids\n\n    return batch\n\nprint(\"Converting to HuggingFace Datasets and processing audio...\")\nprint(\"(This will take a while for 44K+ training samples)\")\nprint(\"NOTE: Using single process to avoid OOM subprocess crashes.\")\n\n# Convert DataFrames to Datasets\ntrain_dataset = Dataset.from_pandas(train_df[[\"audio_path\", \"transcription\"]])\ntest_dataset = Dataset.from_pandas(test_df[[\"audio_path\", \"transcription\"]])\nval_dataset = Dataset.from_pandas(validation_df[[\"audio_path\", \"transcription\"]])\n\n# Process datasets ‚Äî single process to avoid OOM crashes\n# On Colab with high-RAM runtime, you can try num_proc=2\ntrain_dataset = train_dataset.map(\n    prepare_dataset,\n    remove_columns=[\"audio_path\", \"transcription\"],\n)\ntest_dataset = test_dataset.map(\n    prepare_dataset,\n    remove_columns=[\"audio_path\", \"transcription\"],\n)\nval_dataset = val_dataset.map(\n    prepare_dataset,\n    remove_columns=[\"audio_path\", \"transcription\"],\n)\n\nprint(f\"Train dataset: {len(train_dataset)} samples\")\nprint(f\"Test dataset:  {len(test_dataset)} samples\")\nprint(f\"Val dataset:   {len(val_dataset)} samples\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:43:38.883968Z","iopub.execute_input":"2026-02-19T00:43:38.884714Z","iopub.status.idle":"2026-02-19T00:54:56.735578Z","shell.execute_reply.started":"2026-02-19T00:43:38.884681Z","shell.execute_reply":"2026-02-19T00:54:56.734595Z"}},"outputs":[{"name":"stdout","text":"Converting to HuggingFace Datasets and processing audio...\n(This will take a while for 44K+ training samples)\nNOTE: Using single process to avoid OOM subprocess crashes.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/44882 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a081bef75be422e95717c128b3798fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2640 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2df2eb4c47964ffcb1c995f94501da45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a542bf181cd44f8eab4f904163caa943"}},"metadata":{}},{"name":"stdout","text":"Train dataset: 44882 samples\nTest dataset:  2640 samples\nVal dataset:   400 samples\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# üèóÔ∏è Section 7: Model Setup\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    AutoConfig,\n)\n\n# Load model with CTC head\n# We configure the CTC head to match our vocabulary\nconfig = AutoConfig.from_pretrained(BASE_MODEL)\nconfig.update({\n    \"vocab_size\": len(processor.tokenizer),\n    \"ctc_loss_reduction\": \"mean\",\n    \"pad_token_id\": processor.tokenizer.pad_token_id,\n    \"ctc_zero_infinity\": True,  # Prevents NaN loss\n    # SpecAugment configuration (data augmentation during training)\n    \"mask_time_prob\": 0.05,\n    \"mask_time_length\": 10,\n    \"mask_feature_prob\": 0.004,\n    \"mask_feature_length\": 10,\n    # Regularization\n    \"attention_dropout\": 0.1,\n    \"hidden_dropout\": 0.1,\n    \"feat_proj_dropout\": 0.0,\n    \"layerdrop\": 0.1,\n})\n\nmodel = Wav2Vec2ForCTC.from_pretrained(\n    BASE_MODEL,\n    config=config,\n    ignore_mismatched_sizes=True,  # CTC head size differs\n)\n\n# *** CRITICAL: Freeze the feature encoder ***\nif FREEZE_FEATURE_ENCODER:\n    model.freeze_feature_encoder()\n    print(\"‚úÖ Feature encoder FROZEN (only transformer + CTC head will be fine-tuned)\")\n\n# Print trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total parameters:     {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\nprint(f\"Frozen parameters:    {total_params - trainable_params:,}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:54:56.737035Z","iopub.execute_input":"2026-02-19T00:54:56.738075Z","iopub.status.idle":"2026-02-19T00:55:04.192438Z","shell.execute_reply.started":"2026-02-19T00:54:56.738031Z","shell.execute_reply":"2026-02-19T00:55:04.191388Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a7a0c08b328456f80fffa3362422361"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cb46d0ea47347fdabd44c6186215d66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19505d3f2e4e43da9ed827c044dc0445"}},"metadata":{}},{"name":"stderr","text":"Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.bias', 'lm_head.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Feature encoder FROZEN (only transformer + CTC head will be fine-tuned)\nTotal parameters:     315,509,445\nTrainable parameters: 311,299,269\nFrozen parameters:    4,210,176\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# üì¶ Section 8: Data Collator\n","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\nimport torch\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n    \"\"\"\n    Data collator that dynamically pads the inputs and labels.\n    - Pads input_values to the longest in the batch\n    - Pads labels to the longest in the batch (with -100 for CTC ignore)\n    \"\"\"\n    processor: Wav2Vec2Processor\n    padding: Union[bool, str] = True\n    max_length: Optional[int] = None\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # Separate input_values and labels\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        # Pad inputs\n        batch = self.processor.feature_extractor.pad(\n            input_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n\n        # Pad labels\n        labels_batch = self.processor.tokenizer.pad(\n            label_features,\n            padding=self.padding,\n            return_tensors=\"pt\",\n        )\n\n        # Replace padding token id with -100 so CTC loss ignores them\n        labels = labels_batch[\"input_ids\"].masked_fill(\n            labels_batch.attention_mask.ne(1), -100\n        )\n\n        batch[\"labels\"] = labels\n        return batch\n\ndata_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\nprint(\"‚úÖ Data collator ready\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:04.193924Z","iopub.execute_input":"2026-02-19T00:55:04.194921Z","iopub.status.idle":"2026-02-19T00:55:04.213531Z","shell.execute_reply.started":"2026-02-19T00:55:04.194873Z","shell.execute_reply":"2026-02-19T00:55:04.212393Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Data collator ready\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# üìà Section 9: Metrics (WER + CER)\n","metadata":{}},{"cell_type":"code","source":"!pip install evaluate \n!pip install jiwer\nfrom IPython.display import clear_output\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:04.215118Z","iopub.execute_input":"2026-02-19T00:55:04.215582Z","iopub.status.idle":"2026-02-19T00:55:30.171118Z","shell.execute_reply.started":"2026-02-19T00:55:04.215539Z","shell.execute_reply":"2026-02-19T00:55:30.170121Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import evaluate\n\nwer_metric = evaluate.load(\"wer\")\ncer_metric = evaluate.load(\"cer\")\n\ndef compute_metrics(pred):\n    \"\"\"Compute WER and CER during evaluation.\"\"\"\n    pred_logits = pred.predictions\n    pred_ids = np.argmax(pred_logits, axis=-1)\n\n    # Replace -100 with pad token id for decoding\n    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n\n    # Decode predictions and references\n    pred_str = processor.batch_decode(pred_ids)\n    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n\n    # Filter out empty strings\n    filtered = [(p, l) for p, l in zip(pred_str, label_str) if len(l.strip()) > 0]\n    if not filtered:\n        return {\"wer\": 1.0, \"cer\": 1.0}\n    pred_str, label_str = zip(*filtered)\n\n    wer = wer_metric.compute(predictions=list(pred_str), references=list(label_str))\n    cer = cer_metric.compute(predictions=list(pred_str), references=list(label_str))\n\n    return {\"wer\": wer, \"cer\": cer}\n\nprint(\"‚úÖ Metrics ready (WER + CER)\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:30.172697Z","iopub.execute_input":"2026-02-19T00:55:30.173014Z","iopub.status.idle":"2026-02-19T00:55:31.949392Z","shell.execute_reply.started":"2026-02-19T00:55:30.172977Z","shell.execute_reply":"2026-02-19T00:55:31.948537Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b85c8525333841f684e6b5682e7d5451"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5890098ad554bb091dce8af65af1925"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Metrics ready (WER + CER)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# üöÄ Section 10: Training\n","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n    learning_rate=LEARNING_RATE,\n    warmup_ratio=WARMUP_RATIO,\n    weight_decay=0.005,\n    fp16=True,  # Mixed precision training\n\n    # *** CRITICAL: group_by_length reduces padding waste ***\n    group_by_length=True,\n    length_column_name=\"input_length\",\n\n    # Evaluation & saving strategy\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    save_total_limit=3,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n\n    # Performance\n    dataloader_num_workers=4,\n    remove_unused_columns=False,\n    report_to=\"none\",\n    logging_dir=LOGGING_DIR,\n    disable_tqdm=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,  # Use VALIDATION set for eval, NOT test set\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n)\n\nprint(\"‚úÖ Trainer configured\")\nprint(f\"   Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\nprint(f\"   Total training steps: ~{len(train_dataset) * NUM_EPOCHS // (BATCH_SIZE * GRADIENT_ACCUMULATION):,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:31.950741Z","iopub.execute_input":"2026-02-19T00:55:31.951116Z","iopub.status.idle":"2026-02-19T00:55:35.576641Z","shell.execute_reply.started":"2026-02-19T00:55:31.951077Z","shell.execute_reply":"2026-02-19T00:55:35.575743Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_151/1948109901.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Trainer configured\n   Effective batch size: 32\n   Total training steps: ~42,076\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Start training!\nprint(\"üèãÔ∏è Starting training...\")\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:35.577709Z","iopub.execute_input":"2026-02-19T00:55:35.578064Z","iopub.status.idle":"2026-02-19T00:55:56.817331Z","shell.execute_reply.started":"2026-02-19T00:55:35.578016Z","shell.execute_reply":"2026-02-19T00:55:56.814834Z"}},"outputs":[{"name":"stdout","text":"üèãÔ∏è Starting training...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_151/3550170483.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start training!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üèãÔ∏è Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4108\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4109\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4110\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4111\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4112\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     ) -> list[Any]:\n\u001b[0;32m--> 213\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# be constructed, don't try to instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/parallel_apply.py\", line 99, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 1862, in forward\n    outputs = self.wav2vec2(\n              ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 1462, in forward\n    encoder_outputs = self.encoder(\n                      ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 826, in forward\n    layer_outputs = layer(\n                    ^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n    return super().__call__(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 667, in forward\n    hidden_states = self.dropout(hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py\", line 70, in forward\n    return F.dropout(input, self.p, self.training, self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\", line 1422, in dropout\n    _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 1 has a total capacity of 14.56 GiB of which 5.81 MiB is free. Including non-PyTorch memory, this process has 14.55 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 329.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"],"ename":"OutOfMemoryError","evalue":"Caught OutOfMemoryError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/parallel_apply.py\", line 99, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 1862, in forward\n    outputs = self.wav2vec2(\n              ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 1462, in forward\n    encoder_outputs = self.encoder(\n                      ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 826, in forward\n    layer_outputs = layer(\n                    ^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n    return super().__call__(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\", line 667, in forward\n    hidden_states = self.dropout(hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py\", line 70, in forward\n    return F.dropout(input, self.p, self.training, self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\", line 1422, in dropout\n    _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 1 has a total capacity of 14.56 GiB of which 5.81 MiB is free. Including non-PyTorch memory, this process has 14.55 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 329.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"# Save the best model and processor\ntrainer.save_model(os.path.join(OUTPUT_DIR, \"best_model\"))\nprocessor.save_pretrained(os.path.join(OUTPUT_DIR, \"best_model\"))\nprint(f\"‚úÖ Best model saved to {OUTPUT_DIR}/best_model\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.818878Z","iopub.status.idle":"2026-02-19T00:55:56.819318Z","shell.execute_reply.started":"2026-02-19T00:55:56.819104Z","shell.execute_reply":"2026-02-19T00:55:56.819132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üî§ Section 11: KenLM Language Model (THE KEY TO FIXING WER)\n","metadata":{}},{"cell_type":"code","source":"# This is the MOST IMPORTANT section for improving WER.\n# A language model guides beam search to produce valid Telugu words.\n\nos.makedirs(LM_DIR, exist_ok=True)\n\n# Step 1: Prepare Telugu text corpus for LM training\n# Combine all transcriptions + any external Telugu text you have\nlm_texts = (\n    train_df[\"transcription\"].tolist() +\n    test_df[\"transcription\"].tolist() +\n    validation_df[\"transcription\"].tolist()\n)\n\n# Write text corpus (one sentence per line)\nlm_corpus_path = os.path.join(LM_DIR, \"telugu_corpus.txt\")\nwith open(lm_corpus_path, \"w\", encoding=\"utf-8\") as f:\n    for text in lm_texts:\n        text = text.strip()\n        if len(text) > 0:\n            f.write(text + \"\\n\")\n\nprint(f\"LM corpus: {len(lm_texts):,} sentences written to {lm_corpus_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.820651Z","iopub.status.idle":"2026-02-19T00:55:56.821110Z","shell.execute_reply.started":"2026-02-19T00:55:56.820883Z","shell.execute_reply":"2026-02-19T00:55:56.820912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Train KenLM n-gram model\n# Install kenlm if not already: pip install https://github.com/kpu/kenlm/archive/master.zip\n# Or use pre-built: pip install kenlm\n\n# NOTE: You need the kenlm binary 'lmplz' installed.\n# On Kaggle/Colab, install with:\n# !apt-get install -y build-essential cmake libboost-all-dev\n# !pip install https://github.com/kpu/kenlm/archive/master.zip\n\nlm_arpa_path = os.path.join(LM_DIR, \"telugu_5gram.arpa\")\n\n# Train 5-gram LM using KenLM\n# If kenlm binary is available:\nimport subprocess\ntry:\n    subprocess.run(\n        f\"lmplz -o 5 --prune 0 1 1 1 1 < {lm_corpus_path} > {lm_arpa_path}\",\n        shell=True, check=True\n    )\n    print(f\"‚úÖ 5-gram LM trained: {lm_arpa_path}\")\nexcept FileNotFoundError:\n    print(\"‚ö†Ô∏è  'lmplz' not found. Install KenLM or use the alternative below.\")\n    print(\"   Alternative: pip install pyctcdecode[kenlm]\")\n    print(\"   Then use: from pyctcdecode import build_ctcdecoder\")\n    print(\"   The decoder can work without an LM, just with worse WER.\")\n    lm_arpa_path = None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.823028Z","iopub.status.idle":"2026-02-19T00:55:56.823358Z","shell.execute_reply.started":"2026-02-19T00:55:56.823185Z","shell.execute_reply":"2026-02-19T00:55:56.823232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Build CTC decoder with LM\nfrom pyctcdecode import build_ctcdecoder\n\n# Get vocabulary labels in the correct order\nvocab_dict_sorted = dict(sorted(processor.tokenizer.get_vocab().items(), key=lambda x: x[1]))\nlabels = list(vocab_dict_sorted.keys())\n\n# Build decoder\nif lm_arpa_path and os.path.exists(lm_arpa_path):\n    decoder = build_ctcdecoder(\n        labels=labels,\n        kenlm_model_path=lm_arpa_path,\n        alpha=0.5,   # LM weight (tune this: 0.1 - 1.0)\n        beta=1.5,    # Word insertion bonus (tune this: 0.5 - 3.0)\n    )\n    print(\"‚úÖ CTC decoder with KenLM ready\")\nelse:\n    decoder = build_ctcdecoder(labels=labels)\n    print(\"‚ö†Ô∏è CTC decoder WITHOUT LM (greedy beam search)\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.824184Z","iopub.status.idle":"2026-02-19T00:55:56.824506Z","shell.execute_reply.started":"2026-02-19T00:55:56.824363Z","shell.execute_reply":"2026-02-19T00:55:56.824381Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üß™ Section 12: Evaluation with Beam Search Decoding\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchaudio\nimport jiwer\nfrom tqdm import tqdm\n\ndef evaluate_with_beam_search(model, dataset_df, processor, decoder, beam_width=100):\n    \"\"\"\n    Evaluate the model using beam search + LM decoding.\n    This is where the WER magic happens.\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n\n    all_predictions = []\n    all_references = []\n\n    wer_transform = jiwer.Compose([\n        jiwer.Strip(),\n        jiwer.RemoveMultipleSpaces(),\n    ])\n\n    for idx in tqdm(range(len(dataset_df)), desc=\"Evaluating\"):\n        row = dataset_df.iloc[idx]\n        audio_path = row[\"audio_path\"]\n        reference = row[\"transcription\"]\n\n        # Load audio\n        waveform, sample_rate = torchaudio.load(audio_path)\n        if sample_rate != SAMPLING_RATE:\n            waveform = torchaudio.transforms.Resample(sample_rate, SAMPLING_RATE)(waveform)\n\n        # Truncate\n        max_samples = int(MAX_AUDIO_LENGTH_SECONDS * SAMPLING_RATE)\n        if waveform.shape[1] > max_samples:\n            waveform = waveform[:, :max_samples]\n\n        # Process\n        input_values = processor(\n            waveform.squeeze().numpy(),\n            sampling_rate=SAMPLING_RATE,\n            return_tensors=\"pt\",\n        ).input_values.to(device)\n\n        # Get logits\n        with torch.no_grad():\n            logits = model(input_values).logits\n\n        logits_np = logits.cpu().numpy()[0]\n\n        # *** BEAM SEARCH WITH LM ***\n        prediction = decoder.decode(logits_np, beam_width=beam_width)\n\n        all_predictions.append(wer_transform(prediction))\n        all_references.append(wer_transform(reference))\n\n    # Compute metrics\n    wer = jiwer.wer(all_references, all_predictions)\n    cer = jiwer.cer(all_references, all_predictions)\n\n    return wer, cer, all_predictions, all_references\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.826244Z","iopub.status.idle":"2026-02-19T00:55:56.826687Z","shell.execute_reply.started":"2026-02-19T00:55:56.826462Z","shell.execute_reply":"2026-02-19T00:55:56.826492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate on VALIDATION set with beam search + LM\nprint(\"=\" * 60)\nprint(\"EVALUATION: Beam Search + Language Model Decoding\")\nprint(\"=\" * 60)\n\nwer_beam, cer_beam, preds_beam, refs_beam = evaluate_with_beam_search(\n    model=model,\n    dataset_df=validation_df,\n    processor=processor,\n    decoder=decoder,\n    beam_width=100,\n)\n\nprint(f\"\\n{'='*40}\")\nprint(f\"  WER (Beam Search + LM): {wer_beam:.4f} ({wer_beam*100:.2f}%)\")\nprint(f\"  CER (Beam Search + LM): {cer_beam:.4f} ({cer_beam*100:.2f}%)\")\nprint(f\"{'='*40}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.828464Z","iopub.status.idle":"2026-02-19T00:55:56.828840Z","shell.execute_reply.started":"2026-02-19T00:55:56.828663Z","shell.execute_reply":"2026-02-19T00:55:56.828700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare with greedy decoding (what you had before)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"COMPARISON: Greedy Decoding (your old method)\")\nprint(\"=\" * 60)\n\nmodel.eval()\ndevice = next(model.parameters()).device\n\ngreedy_preds = []\ngreedy_refs = []\n\nfor idx in tqdm(range(len(validation_df)), desc=\"Greedy eval\"):\n    row = validation_df.iloc[idx]\n    waveform, sr = torchaudio.load(row[\"audio_path\"])\n    if sr != SAMPLING_RATE:\n        waveform = torchaudio.transforms.Resample(sr, SAMPLING_RATE)(waveform)\n    max_samples = int(MAX_AUDIO_LENGTH_SECONDS * SAMPLING_RATE)\n    if waveform.shape[1] > max_samples:\n        waveform = waveform[:, :max_samples]\n\n    input_values = processor(\n        waveform.squeeze().numpy(), sampling_rate=SAMPLING_RATE, return_tensors=\"pt\"\n    ).input_values.to(device)\n\n    with torch.no_grad():\n        logits = model(input_values).logits\n    pred_ids = torch.argmax(logits, dim=-1)\n    prediction = processor.decode(pred_ids[0])\n\n    greedy_preds.append(prediction.strip())\n    greedy_refs.append(row[\"transcription\"].strip())\n\nwer_greedy = jiwer.wer(greedy_refs, greedy_preds)\ncer_greedy = jiwer.cer(greedy_refs, greedy_preds)\n\nprint(f\"\\n{'='*50}\")\nprint(f\"  GREEDY:      WER={wer_greedy:.4f} ({wer_greedy*100:.2f}%)  CER={cer_greedy:.4f} ({cer_greedy*100:.2f}%)\")\nprint(f\"  BEAM+LM:     WER={wer_beam:.4f} ({wer_beam*100:.2f}%)  CER={cer_beam:.4f} ({cer_beam*100:.2f}%)\")\nprint(f\"  IMPROVEMENT:  WER={wer_greedy - wer_beam:.4f} ({(wer_greedy - wer_beam)*100:.2f}% absolute)\")\nprint(f\"{'='*50}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.830116Z","iopub.status.idle":"2026-02-19T00:55:56.830560Z","shell.execute_reply.started":"2026-02-19T00:55:56.830346Z","shell.execute_reply":"2026-02-19T00:55:56.830375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show sample predictions\nprint(\"\\n--- Sample Predictions (Beam+LM vs Greedy) ---\")\nfor i in range(min(10, len(validation_df))):\n    print(f\"\\n[{i}] Reference:  {greedy_refs[i]}\")\n    print(f\"    Greedy:     {greedy_preds[i]}\")\n    print(f\"    Beam+LM:    {preds_beam[i]}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.831738Z","iopub.status.idle":"2026-02-19T00:55:56.832152Z","shell.execute_reply.started":"2026-02-19T00:55:56.831933Z","shell.execute_reply":"2026-02-19T00:55:56.831964Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üîç Section 13: LM Hyperparameter Tuning (alpha & beta)\n","metadata":{}},{"cell_type":"code","source":"# Tune alpha (LM weight) and beta (word insertion bonus)\n# This can improve WER by another 2-5%\n\nprint(\"Tuning LM hyperparameters on validation set...\")\nprint(\"(This takes a while - skip if you're in a hurry)\")\n\nbest_wer = float(\"inf\")\nbest_alpha = 0.5\nbest_beta = 1.5\n\n# Quick tuning grid\nalphas = [0.1, 0.3, 0.5, 0.7, 1.0]\nbetas = [0.5, 1.0, 1.5, 2.0, 3.0]\n\nif lm_arpa_path and os.path.exists(lm_arpa_path):\n    for alpha in alphas:\n        for beta in betas:\n            # Rebuild decoder with new params\n            test_decoder = build_ctcdecoder(\n                labels=labels,\n                kenlm_model_path=lm_arpa_path,\n                alpha=alpha,\n                beta=beta,\n            )\n\n            # Quick eval on first 50 samples for speed\n            quick_preds = []\n            quick_refs = []\n            for idx in range(min(50, len(validation_df))):\n                row = validation_df.iloc[idx]\n                waveform, sr = torchaudio.load(row[\"audio_path\"])\n                if sr != SAMPLING_RATE:\n                    waveform = torchaudio.transforms.Resample(sr, SAMPLING_RATE)(waveform)\n                input_values = processor(\n                    waveform.squeeze().numpy(), sampling_rate=SAMPLING_RATE, return_tensors=\"pt\"\n                ).input_values.to(device)\n                with torch.no_grad():\n                    logits = model(input_values).logits\n                pred = test_decoder.decode(logits.cpu().numpy()[0], beam_width=50)\n                quick_preds.append(pred.strip())\n                quick_refs.append(row[\"transcription\"].strip())\n\n            wer = jiwer.wer(quick_refs, quick_preds)\n            if wer < best_wer:\n                best_wer = wer\n                best_alpha = alpha\n                best_beta = beta\n                print(f\"  New best: alpha={alpha}, beta={beta}, WER={wer:.4f}\")\n\n    print(f\"\\n‚úÖ Best LM params: alpha={best_alpha}, beta={best_beta}, WER={best_wer:.4f}\")\n\n    # Rebuild decoder with best params\n    decoder = build_ctcdecoder(\n        labels=labels,\n        kenlm_model_path=lm_arpa_path,\n        alpha=best_alpha,\n        beta=best_beta,\n    )\nelse:\n    print(\"‚ö†Ô∏è Skipping - no LM available\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.835090Z","iopub.status.idle":"2026-02-19T00:55:56.835522Z","shell.execute_reply.started":"2026-02-19T00:55:56.835376Z","shell.execute_reply":"2026-02-19T00:55:56.835396Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üêã Section 14: Whisper Fine-Tuning (Comparison Baseline)\n\nWhisper uses attention-based seq2seq decoding, NOT CTC.\nThis avoids many CTC blank/spike issues and handles word boundaries natively.\nThis section is OPTIONAL but strongly recommended for a research paper.\n","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    WhisperProcessor,\n    WhisperForConditionalGeneration,\n    WhisperTokenizer,\n    WhisperFeatureExtractor,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n)\n\nWHISPER_MODEL = \"openai/whisper-small\"  # or \"openai/whisper-medium\" for better results\nWHISPER_OUTPUT_DIR = \"./results_whisper\"\n\n# Load Whisper components\nwhisper_feature_extractor = WhisperFeatureExtractor.from_pretrained(WHISPER_MODEL)\nwhisper_tokenizer = WhisperTokenizer.from_pretrained(WHISPER_MODEL, language=\"te\", task=\"transcribe\")\nwhisper_processor = WhisperProcessor.from_pretrained(WHISPER_MODEL, language=\"te\", task=\"transcribe\")\nwhisper_model = WhisperForConditionalGeneration.from_pretrained(WHISPER_MODEL)\n\n# Set language and task\nwhisper_model.generation_config.language = \"te\"\nwhisper_model.generation_config.task = \"transcribe\"\nwhisper_model.generation_config.forced_decoder_ids = None\n\nprint(f\"‚úÖ Whisper model loaded: {WHISPER_MODEL}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.836990Z","iopub.status.idle":"2026-02-19T00:55:56.837682Z","shell.execute_reply.started":"2026-02-19T00:55:56.837530Z","shell.execute_reply":"2026-02-19T00:55:56.837550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare Whisper dataset\ndef prepare_whisper_dataset(batch):\n    \"\"\"Process audio for Whisper.\"\"\"\n    audio, sr = librosa.load(batch[\"audio_path\"], sr=SAMPLING_RATE)\n\n    # Truncate\n    max_samples = int(MAX_AUDIO_LENGTH_SECONDS * SAMPLING_RATE)\n    if len(audio) > max_samples:\n        audio = audio[:max_samples]\n\n    batch[\"input_features\"] = whisper_feature_extractor(\n        audio, sampling_rate=SAMPLING_RATE\n    ).input_features[0]\n\n    batch[\"labels\"] = whisper_tokenizer(batch[\"transcription\"]).input_ids\n    return batch\n\nwhisper_train = Dataset.from_pandas(train_df[[\"audio_path\", \"transcription\"]])\nwhisper_val = Dataset.from_pandas(validation_df[[\"audio_path\", \"transcription\"]])\n\nwhisper_train = whisper_train.map(\n    prepare_whisper_dataset,\n    remove_columns=[\"audio_path\", \"transcription\"],\n    num_proc=4,\n)\nwhisper_val = whisper_val.map(\n    prepare_whisper_dataset,\n    remove_columns=[\"audio_path\", \"transcription\"],\n    num_proc=2,\n)\n\nprint(f\"Whisper train: {len(whisper_train)}, val: {len(whisper_val)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.838744Z","iopub.status.idle":"2026-02-19T00:55:56.839161Z","shell.execute_reply.started":"2026-02-19T00:55:56.838970Z","shell.execute_reply":"2026-02-19T00:55:56.838998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Whisper data collator\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: WhisperProcessor\n    decoder_start_token_id: int\n\n    def __call__(self, features):\n        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n        labels = labels_batch[\"input_ids\"].masked_fill(\n            labels_batch.attention_mask.ne(1), -100\n        )\n        if (labels[:, 0] == self.decoder_start_token_id).all():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n        return batch\n\nwhisper_collator = DataCollatorSpeechSeq2SeqWithPadding(\n    processor=whisper_processor,\n    decoder_start_token_id=whisper_model.config.decoder_start_token_id,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.840313Z","iopub.status.idle":"2026-02-19T00:55:56.840707Z","shell.execute_reply.started":"2026-02-19T00:55:56.840544Z","shell.execute_reply":"2026-02-19T00:55:56.840567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Whisper compute metrics\ndef compute_whisper_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n    label_ids[label_ids == -100] = whisper_tokenizer.pad_token_id\n\n    pred_str = whisper_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = whisper_tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    wer = jiwer.wer(label_str, pred_str)\n    cer = jiwer.cer(label_str, pred_str)\n    return {\"wer\": wer, \"cer\": cer}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.842070Z","iopub.status.idle":"2026-02-19T00:55:56.842426Z","shell.execute_reply.started":"2026-02-19T00:55:56.842283Z","shell.execute_reply":"2026-02-19T00:55:56.842306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Whisper training\nwhisper_training_args = Seq2SeqTrainingArguments(\n    output_dir=WHISPER_OUTPUT_DIR,\n    num_train_epochs=10,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=8,\n    learning_rate=1e-5,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    fp16=True,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    predict_with_generate=True,\n    generation_max_length=225,\n    logging_steps=50,\n    report_to=\"none\",\n    remove_unused_columns=False,\n)\n\nwhisper_trainer = Seq2SeqTrainer(\n    model=whisper_model,\n    args=whisper_training_args,\n    train_dataset=whisper_train,\n    eval_dataset=whisper_val,\n    data_collator=whisper_collator,\n    compute_metrics=compute_whisper_metrics,\n    tokenizer=whisper_processor.feature_extractor,\n)\n\nprint(\"üèãÔ∏è Starting Whisper training...\")\nwhisper_trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.843950Z","iopub.status.idle":"2026-02-19T00:55:56.844353Z","shell.execute_reply.started":"2026-02-19T00:55:56.844118Z","shell.execute_reply":"2026-02-19T00:55:56.844147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate Whisper\nwhisper_results = whisper_trainer.evaluate()\nprint(f\"\\n{'='*50}\")\nprint(f\"  WHISPER Results:\")\nprint(f\"  WER: {whisper_results['eval_wer']:.4f} ({whisper_results['eval_wer']*100:.2f}%)\")\nprint(f\"  CER: {whisper_results['eval_cer']:.4f} ({whisper_results['eval_cer']*100:.2f}%)\")\nprint(f\"{'='*50}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.846408Z","iopub.status.idle":"2026-02-19T00:55:56.846811Z","shell.execute_reply.started":"2026-02-19T00:55:56.846610Z","shell.execute_reply":"2026-02-19T00:55:56.846637Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìä Section 15: Final Comparison Table\n","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 70)\nprint(\"  FINAL RESULTS COMPARISON\")\nprint(\"=\" * 70)\nprint(f\"{'Model':<35} {'WER':>8} {'CER':>8}\")\nprint(\"-\" * 53)\nprint(f\"{'Wav2Vec2 Greedy':<35} {wer_greedy*100:>7.2f}% {cer_greedy*100:>7.2f}%\")\nprint(f\"{'Wav2Vec2 Beam+LM':<35} {wer_beam*100:>7.2f}% {cer_beam*100:>7.2f}%\")\ntry:\n    print(f\"{'Whisper (seq2seq)':<35} {whisper_results['eval_wer']*100:>7.2f}% {whisper_results['eval_cer']*100:>7.2f}%\")\nexcept:\n    print(f\"{'Whisper (seq2seq)':<35} {'N/A':>8} {'N/A':>8}\")\nprint(\"=\" * 70)\nprint(\"\\nFor your research paper, report all three rows above.\")\nprint(\"The Beam+LM result should show significant WER improvement over Greedy.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T00:55:56.847582Z","iopub.status.idle":"2026-02-19T00:55:56.847998Z","shell.execute_reply.started":"2026-02-19T00:55:56.847779Z","shell.execute_reply":"2026-02-19T00:55:56.847806Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üí° Section 16: Tips for Further Improvement\n\n1. **External Telugu LM data**: Download Telugu Wikipedia dump or IndicNLP corpus\n   and add to your KenLM training. More text = better LM = lower WER.\n\n2. **BPE tokenization**: Try SentencePiece BPE with vocab_size=300-500\n   instead of character-level CTC. This helps group frequent character\n   sequences into subword tokens.\n\n3. **Self-training**: Use your best model to pseudo-label unlabeled Telugu\n   audio, filter by confidence, and retrain.\n\n4. **Data augmentation**: Add speed perturbation (0.9x, 1.1x), noise injection,\n   and room impulse response simulation.\n\n5. **Try IndicConformer**: AI4Bharat's conformer models if available for Telugu.\n\n6. **Ensemble**: Average logits from multiple models before decoding.\n\nprint(\"\\nüéâ Pipeline complete! Check the results above.\")\n","metadata":{}}]}