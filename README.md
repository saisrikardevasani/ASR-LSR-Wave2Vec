# ğŸ™ï¸ Low-Resource Automatic Speech Recognition using Self-Supervised Learning

## ğŸ“Œ Overview

This project focuses on building an **Automatic Speech Recognition (ASR)** system for low-resource Indian languages using state-of-the-art self-supervised speech models.

Low-resource languages often lack sufficient annotated speech data, making traditional ASR approaches ineffective. This project leverages pretrained transformer-based speech models and fine-tunes them on limited labeled datasets to improve transcription performance.

---

## ğŸš€ Key Features

- Fine-tuned Wav2Vec 2.0
- Experimented with WavLM
- Trained on Microsoft Indian Language Speech Corpus
- Focused on Telugu and other low-resource Indian languages
- Conducted dataset scaling experiments
- Analyzed training instability during mid-training steps
- Evaluated performance using Word Error Rate (WER)

---

## ğŸ§  Problem Statement

Building robust ASR systems for low-resource languages is challenging due to:

- Limited annotated speech data
- Pronunciation and dialect variability
- Data imbalance
- Overfitting on small datasets

This project investigates how self-supervised pretrained models can improve ASR performance under such constraints.

---

## ğŸ—ï¸ Architecture

### 1ï¸âƒ£ Pretrained Backbone
- Wav2Vec 2.0 / WavLM (Transformer-based encoders)

### 2ï¸âƒ£ Fine-Tuning Strategy
- CTC (Connectionist Temporal Classification) Loss
- HuggingFace Transformers framework
- Gradient accumulation for memory efficiency

### 3ï¸âƒ£ Training Configuration
- Variable dataset sizes
- Learning rate scheduling
- Mixed precision training
- Early stopping experiments

---

## ğŸ“Š Experiments

### Dataset Scaling Study

| Dataset Size | Observation |
|--------------|------------|
| Small subset | Faster convergence but high overfitting |
| Medium subset | Improved generalization |
| Larger subset | Lower WER but increased training time |

### Training Instability Analysis

Observed behavior around 2000â€“2400 steps:
- Temporary increase in validation loss
- Possible causes:
  - Learning rate adjustments
  - Overfitting
  - Gradient instability
  - Data distribution shifts

This analysis helped refine training strategy and hyperparameter tuning.

---

## ğŸ“ˆ Evaluation Metrics

- Word Error Rate (WER)
- Validation Loss Tracking
- Qualitative Transcription Review

---

## ğŸ› ï¸ Tech Stack

- Python
- PyTorch
- HuggingFace Transformers
- Torchaudio
- NumPy
- Pandas
- CUDA-enabled GPU

---

## ğŸ“‚ Project Structure
